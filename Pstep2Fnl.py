import json

def main():
    # Step 1: Chunk the extracted text
    with open("./extracted_content/extracted_text.txt", "r", encoding="utf-8") as f:
        extracted_text = f.read()

    print("Chunking extracted text...")
    chunks = chunk_text(extracted_text)
    print(f"Total chunks created: {len(chunks)}")

    # Step 2: Process chunks concurrently
    print("Processing chunks concurrently...")
    results = process_chunks_concurrently(chunks)
    print(f"Processed {len(results)} chunks.")

    # Step 3: Consolidate results across all chunks
    print("Consolidating results...")
    final_results = consolidate_results(results)

    # Save consolidated results for inspection/debugging
    with open("./consolidated_whitepaper_analysis.json", "w", encoding="utf-8") as f:
        json.dump(final_results, f, indent=2)
    
    print("Consolidated whitepaper analysis saved.")

    # Step 4: Clean and summarize consolidated results
    print("Cleaning and summarizing consolidated results...")
    cleaned_final_results = clean_and_summarize_final_results(final_results)

    # Save cleaned results for inspection/debugging
    with open("./cleaned_whitepaper_analysis.json", "w", encoding="utf-8") as f:
        json.dump(cleaned_final_results, f, indent=2)
    
    print("Cleaned whitepaper analysis saved.")

    # Step 5: Export cleaned results to Word document
    output_file_path = "./cleaned_whitepaper_analysis.docx"
    export_to_word(cleaned_final_results, output_file_path)

    print(f"Cleaned whitepaper analysis exported to {output_file_path}.")

# Run the workflow
if __name__ == "__main__":
    main()









# Step 3: Consolidate results across all chunks
final_results = consolidate_results(results)

# Save consolidated results to a JSON file for inspection/debugging
import json
with open("./consolidated_whitepaper_analysis.json", "w", encoding="utf-8") as f:
    json.dump(final_results, f, indent=2)

print("Consolidated whitepaper analysis saved.")

# Step 4: Clean and summarize consolidated results
cleaned_final_results = clean_and_summarize_final_results(final_results)

# Save cleaned results to a JSON file for inspection/debugging
with open("./cleaned_whitepaper_analysis.json", "w", encoding="utf-8") as f:
    json.dump(cleaned_final_results, f, indent=2)

print("Cleaned whitepaper analysis saved.")



# Step 5: Export cleaned results to Word document
output_file_path = "./cleaned_whitepaper_analysis.docx"
export_to_word(cleaned_final_results, output_file_path)

print(f"Cleaned whitepaper analysis exported to {output_file_path}.")









# Load consolidated results from JSON file (or use your existing results object)
with open("./consolidated_whitepaper_analysis.json", "r", encoding="utf-8") as f:
   consolidated_results = json.load(f)

# Clean and summarize results
cleaned_final_results = clean_and_summarize_final_results(consolidated_results)

# Save cleaned results to Word document
output_file_path = "./cleaned_whitepaper_analysis.docx"
export_to_word(cleaned_final_results, output_file_path)

print(f"Cleaned whitepaper analysis saved to {output_file_path}")








def summarize_consolidated_summaries(summaries):
    """
    Uses Claude via AWS Bedrock to generate a high-level summary from consolidated summaries.
    
    Args:
        summaries (list): List of individual summaries.

    Returns:
        str: High-level summary generated by Claude.
    """
    if not summaries:
        return "No summaries available to summarize."

    bedrock_client = boto3.client('bedrock-runtime', region_name='us-east-1')
    
    # Combine all summaries into a single string
    combined_summaries = "\n".join([f"{i + 1}. {summary}" for i, summary in enumerate(summaries)])
    
    prompt = f"""
    You are an expert tasked with summarizing a whitepaper analysis report.
    
    Below are individual summaries extracted from different sections of the whitepaper:

    {combined_summaries}

    Generate a high-level summary that captures key insights and avoids redundancy.
    Ensure your response is concise and structured as a single paragraph.
    """
    
    try:
        response = bedrock_client.invoke_model(
            modelId="anthropic.claude-3-haiku-20240307-v1:0",
            body=json.dumps({
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 2048,
                "temperature": 0.7,
                "top_p": 0.9,
            })
        )
        
        response_body = json.loads(response['body'].read().decode('utf-8'))
        return response_body["content"].strip()
    
    except Exception as e:
        logging.error(f"Error generating high-level summary: {str(e)}")
        return f"Failed to generate high-level summary: {str(e)}"



def deduplicate_section(section, key=None):
    """
    Deduplicates entries in a section based on a specific key or the entire content.
    
    Args:
        section (list): List of dictionaries or strings representing the section content.
        key (str): Key to deduplicate by (e.g., 'name' for Inputs/Outputs). If None, deduplicates by full content.

    Returns:
        list: Deduplicated section content.
    """
    seen = set()
    deduplicated = []
    
    for item in section:
        # Use the specified key for deduplication if provided
        value = item[key] if key and isinstance(item, dict) else str(item)
        
        if value not in seen:
            seen.add(value)
            deduplicated.append(item)
    
    return deduplicated


def clean_and_summarize_final_results(final_results):
    """
    Cleans and summarizes the final consolidated results.
    
    Args:
        final_results (dict): Consolidated structured data from all chunks.

    Returns:
        dict: Cleaned and summarized results.
    """
    # Summarize consolidated summaries
    logging.info("Summarizing consolidated summaries...")
    high_level_summary = summarize_consolidated_summaries(final_results.get("summary", []))
    
    # Deduplicate other sections
    logging.info("Deduplicating other sections...")
    final_results["inputs"] = deduplicate_section(final_results.get("inputs", []), key="name")
    final_results["outputs"] = deduplicate_section(final_results.get("outputs", []), key="name")
    final_results["calculations"] = deduplicate_section(final_results.get("calculations", []))
    final_results["model_performance"] = deduplicate_section(final_results.get("model_performance", []))
    final_results["solution_specification"] = deduplicate_section(final_results.get("solution_specification", []))
    final_results["testing_summary"] = deduplicate_section(final_results.get("testing_summary", []))
    final_results["reconciliation"] = deduplicate_section(final_results.get("reconciliation", []))
    
    # Replace detailed summaries with the high-level summary
    final_results["summary"] = [high_level_summary]
    
    return final_results




from docx import Document
from docx import Document

def export_to_word(final_results, output_file_path):
    """
    Exports consolidated results to a Word document.
    
    Args:
        final_results (dict): Consolidated structured data from all chunks.
        output_file_path (str): Path to save the Word document.
    
    Returns:
        None
    """
    # Create a new Word document
    doc = Document()
    
    # Add title
    doc.add_heading('Whitepaper Analysis Report', level=1)
    
    # Add metadata section
    doc.add_heading('Metadata', level=2)
    metadata = final_results.get("metadata", {})
    doc.add_paragraph(f"Total Chunks: {metadata.get('total_chunks', 0)}")
    doc.add_paragraph(f"Processed Chunks: {metadata.get('processed_chunks', 0)}")
    doc.add_paragraph(f"Failed Chunks: {metadata.get('failed_chunks', 0)}")
    
    if metadata.get("failed_chunk_ids"):
        doc.add_paragraph(f"Failed Chunk IDs: {', '.join(metadata['failed_chunk_ids'])}")
    
    # Add summary section
    doc.add_heading('Summary', level=2)
    summaries = final_results.get("summary", [])
    
    for i, summary in enumerate(summaries):
        doc.add_paragraph(f"{i + 1}. {summary}")
    
    # Add inputs section
    doc.add_heading('Inputs', level=2)
    
    for input_item in final_results.get("inputs", []):
        name = input_item.get("name", "Unknown")
        description = input_item.get("description", "No description available")
        format_type = input_item.get("format", "Unknown format")
        
        doc.add_paragraph(f"- {name}: {description} ({format_type})")
    
    # Add outputs section
    doc.add_heading('Outputs', level=2)
    
    for output_item in final_results.get("outputs", []):
        name = output_item.get("name", "Unknown")
        description = output_item.get("description", "No description available")
        format_type = output_item.get("format", "Unknown format")
        
        doc.add_paragraph(f"- {name}: {description} ({format_type})")
    
    # Add calculations section
    doc.add_heading('Calculations', level=2)
    
    for calculation in final_results.get("calculations", []):
        doc.add_paragraph(f"- {calculation}")
    
    # Add model performance section
    doc.add_heading('Model Performance', level=2)
    
    for performance_metric in final_results.get("model_performance", []):
        doc.add_paragraph(f"- {performance_metric}")
    
    # Add solution specification section
    doc.add_heading('Solution Specification', level=2)
    
    for spec_item in final_results.get("solution_specification", []):
        doc.add_paragraph(f"- {spec_item}")
    
    # Add testing summary section
    doc.add_heading('Testing Summary', level=2)
    
    for test_case in final_results.get("testing_summary", []):
        doc.add_paragraph(f"- {test_case}")
    
    # Add reconciliation section
    doc.add_heading('Reconciliation', level=2)
    
    for reconciliation_process in final_results.get("reconciliation", []):
        doc.add_paragraph(f"- {reconciliation_process}")
    
    # Save the document
    doc.save(output_file_path)
    print(f"Document saved to: {output_file_path}")

# Example usage
final_results = {
  "summary": [
      "This chunk describes the model's architecture.",
      "This chunk describes additional details about data preprocessing."
  ],
  "inputs": [
      {"name": "input_1", "description": "Historical sales data used to predict future trends.", "format": "numerical"},
      {"name": "input_2", "description": "Customer segmentation based on demographic data.", "format": "categorical"}
  ],
  "outputs": [
      {"name": "output_1", "description": "Predicted sales figures for the next quarter.", "format": "numerical"},
      {"name": "output_2", "description": "Risk scores for customers based on historical behavior.", "format": "numerical"}
  ],
  "calculations": [
      "Linear regression formula applied to sales data.",
      "Clustering algorithm used for customer segmentation."
  ],
  "model_performance": [
      "Accuracy: The model achieves 95% accuracy on historical sales predictions.",
      "Precision: Precision is measured at 90% for high-risk customer identification."
  ],
  "solution_specification": [
      "Architecture: Microservices-based architecture with components for data ingestion and prediction generation.",
      "Components: Includes modules for feature engineering and real-time prediction serving."
  ],
  "testing_summary": [
      "Test Case_1: Validates input data preprocessing logic.",
      "Test Case_2: Ensures predictions align with expected trends under stable conditions."
  ],
  "reconciliation": [
      "Matches predicted sales figures against actual sales data from previous quarters.",
      "Validates customer risk scores against historical behavior patterns."
  ],
  "metadata": {
      "total_chunks": 10,
      "processed_chunks": 9,
      "failed_chunks": 1,
      "failed_chunk_ids": ["chunk_3"]
  }
}

output_file_path = "./cleaned_whitepaper_analysis.docx"
export_to_word(final_results, output_file_path)
























pip install python-docx

from docx import Document

def export_to_word(final_results, output_file_path):
    """
    Exports consolidated results to a Word document.
    
    Args:
        final_results (dict): Consolidated structured data from all chunks.
        output_file_path (str): Path to save the Word document.
    
    Returns:
        None
    """
    # Create a new Word document
    doc = Document()
    
    # Add title
    doc.add_heading('Whitepaper Analysis Report', level=1)
    
    # Add metadata section
    doc.add_heading('Metadata', level=2)
    metadata = final_results.get("metadata", {})
    doc.add_paragraph(f"Total Chunks: {metadata.get('total_chunks', 0)}")
    doc.add_paragraph(f"Processed Chunks: {metadata.get('processed_chunks', 0)}")
    doc.add_paragraph(f"Failed Chunks: {metadata.get('failed_chunks', 0)}")
    if metadata.get("failed_chunk_ids"):
        doc.add_paragraph(f"Failed Chunk IDs: {', '.join(metadata['failed_chunk_ids'])}")
    
    # Add summary section
    doc.add_heading('Summary', level=2)
    summaries = final_results.get("summary", [])
    for i, summary in enumerate(summaries):
        doc.add_paragraph(f"{i + 1}. {summary}")
    
    # Add inputs section
    doc.add_heading('Inputs', level=2)
    inputs = final_results.get("inputs", [])
    for input_item in inputs:
        name = input_item.get("name", "Unknown")
        description = input_item.get("description", "No description available")
        format_type = input_item.get("format", "Unknown format")
        doc.add_paragraph(f"- {name}: {description} ({format_type})")
    
    # Add outputs section
    doc.add_heading('Outputs', level=2)
    outputs = final_results.get("outputs", [])
    for output_item in outputs:
        name = output_item.get("name", "Unknown")
        description = output_item.get("description", "No description available")
        format_type = output_item.get("format", "Unknown format")
        doc.add_paragraph(f"- {name}: {description} ({format_type})")
    
    # Add calculations section
    doc.add_heading('Calculations', level=2)
    calculations = final_results.get("calculations", [])
    for calculation in calculations:
        doc.add_paragraph(f"- {calculation}")
    
    # Add model performance section
    doc.add_heading('Model Performance', level=2)
    model_performance = final_results.get("model_performance", [])
    for performance_metric in model_performance:
        doc.add_paragraph(f"- {performance_metric}")
    
    # Add solution specification section
    doc.add_heading('Solution Specification', level=2)
    solution_specification = final_results.get("solution_specification", [])
    for spec_item in solution_specification:
        doc.add_paragraph(f"- {spec_item}")
    
    # Add testing summary section
    doc.add_heading('Testing Summary', level=2)
    testing_summary = final_results.get("testing_summary", [])
    for test_case in testing_summary:
        doc.add_paragraph(f"- {test_case}")
    
    # Add reconciliation section
    doc.add_heading('Reconciliation', level=2)
    reconciliation = final_results.get("reconciliation", [])
    for reconciliation_process in reconciliation:
        doc.add_paragraph(f"- {reconciliation_process}")
    
    # Save the document
    doc.save(output_file_path)
    print(f"Document saved to: {output_file_path}")

# Example usage
final_results = {
  "summary": [
      "This chunk describes the model's architecture.",
      "This chunk describes additional details about data preprocessing."
  ],
  "inputs": [
      {"name": "input_1", "description": "Historical sales data used to predict future trends.", "format": "numerical"},
      {"name": "input_2", "description": "Customer segmentation based on demographic data.", "format": "categorical"}
  ],
  "outputs": [
      {"name": "output_1", "description": "Predicted sales figures for the next quarter.", "format": "numerical"},
      {"name": "output_2", "description": "Risk scores for customers based on historical behavior.", "format": "numerical"}
  ],
  "calculations": [
      "Linear regression formula applied to sales data.",
      "Clustering algorithm used for customer segmentation."
  ],
  "model_performance": [
      "Accuracy: The model achieves 95% accuracy on historical sales predictions.",
      "Precision: Precision is measured at 90% for high-risk customer identification."
  ],
  "solution_specification": [
      "Architecture: Microservices-based architecture with components for data ingestion and prediction generation.",
      "Components: Includes modules for feature engineering and real-time prediction serving."
  ],
  "testing_summary": [
      "Test Case_1: Validates input data preprocessing logic.",
      "Test Case_2: Ensures predictions align with expected trends under stable conditions."
  ],
  "reconciliation": [
      "Matches predicted sales figures against actual sales data from previous quarters.",
      "Validates customer risk scores against historical behavior patterns."
  ],
  "metadata": {
      "total_chunks": 10,
      "processed_chunks": 9,
      "failed_chunks": 1,
      "failed_chunk_ids": ["chunk_3"]
  }
}

output_file_path = "./final_whitepaper_analysis.docx"
export_to_word(final_results, output_file_path)
