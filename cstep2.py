import boto3
import json
import time

def chunk_text(text, chunk_size=8000):
    """
    Splits text into smaller chunks to handle large documents efficiently.
    """
    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]

def analyze_text_with_claude3(text_file):
    """
    Processes extracted text using Claude 3 via AWS Bedrock to generate structured insights
    and stores it in a knowledge base for question answering.
    
    :param text_file: Path to the extracted text file.
    :return: Extracted model details stored as a knowledge base.
    """
    try:
        # Read extracted text
        with open(text_file, "r", encoding="utf-8") as f:
            extracted_text = f.read()
        
        # Initialize Bedrock client
        bedrock = boto3.client(service_name='bedrock-runtime', region_name='us-east-1')  # Change region if needed
        
        # Split text into chunks to avoid token limit issues
        text_chunks = chunk_text(extracted_text)
        
        extracted_insights = []
        
        for i, chunk in enumerate(text_chunks):
            print(f"Processing chunk {i+1}/{len(text_chunks)}...")
            
            # Define Claude 3 prompt
            prompt = f"""
            You are an AI assistant specialized in analyzing complex financial, regulatory, and AI model documentation.
            Analyze the following model whitepaper and extract structured information:
            
            1. **Model Overview:**
               - Model Name
               - Model Version
               - Model Purpose & Scope (Explain in detail)
            
            2. **Model Inputs:**
               - List all required inputs, including data sources, features, and transformations applied.
               - Explain the role of each input in the model.
               
            3. **Model Processing & Calculations:**
               - Detail the core computations, algorithms, and methodologies used.
               - Explain how the model processes the inputs and generates outputs.
               - Mention key mathematical formulas or machine learning techniques involved.
            
            4. **Model Outputs:**
               - List all outputs generated by the model.
               - Define how each output is used in business decisions.
            
            5. **Performance & Evaluation Metrics:**
               - Explain how the modelâ€™s accuracy, robustness, and effectiveness are measured.
               - List key performance metrics and benchmark results.
            
            6. **Assumptions & Limitations:**
               - Detail all underlying assumptions considered during model development.
               - Highlight known limitations and potential risks associated with the model.
            
            7. **Regulatory & Compliance Considerations:**
               - Identify any applicable regulations the model adheres to (e.g., Basel, SR 11-7, GDPR, etc.).
               - List risk controls and governance requirements.
            
            8. **Explainability & Interpretability:**
               - Describe how model decisions can be interpreted.
               - Mention interpretability techniques (e.g., SHAP, LIME, feature importance analysis).
            
            9. **Model Risk & Bias Considerations:**
               - Identify potential sources of model bias.
               - Describe risk mitigation strategies (e.g., fairness constraints, adversarial testing).
               - Explain how model drift and degradation are monitored over time.
            
            10. **Summary of Key Findings:**
               - Provide a concise summary of extracted details.
               - Highlight inconsistencies, gaps, or missing details within the document.
            
            **Important:** Ensure accuracy and completeness when analyzing structured sections. Prioritize factual information from the document, and avoid making assumptions beyond the given text.
            
            **Document Content:**
            {chunk}
            """
            
            payload = {
                "prompt": prompt,
                "max_tokens": 1500,
                "temperature": 0.5
            }
            
            # Invoke Claude 3 with retry logic
            retry_attempts = 3
            for attempt in range(retry_attempts):
                try:
                    response = bedrock.invoke_model(
                        body=json.dumps(payload),
                        modelId='anthropic.claude-3-sonnet-2024-02-29',  # Update model version if needed
                        accept='application/json',
                        contentType='application/json'
                    )
                    
                    response_body = json.loads(response['body'].read().decode('utf-8'))
                    insights = response_body.get('completion', 'No response received')
                    extracted_insights.append({"chunk": i+1, "insights": insights})
                    break  # Exit retry loop if successful
                except Exception as e:
                    print(f"Attempt {attempt+1} failed: {e}")
                    time.sleep(2)  # Wait before retrying
            
        # Save results as a structured knowledge base (JSON)
        kb_file = text_file.replace(".txt", "_knowledge_base.json")
        with open(kb_file, "w", encoding="utf-8") as f:
            json.dump(extracted_insights, f, indent=4)
        
        print(f"Knowledge base saved to {kb_file}")
        return kb_file
    
    except Exception as e:
        print(f"Error analyzing text: {e}")
        return None

# Example usage
extracted_text_file = "extracted_text.txt"  # Ensure this file exists from Step 1
analyze_text_with_claude3(extracted_text_file)
